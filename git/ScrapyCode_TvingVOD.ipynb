{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 티빙클립 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프로젝트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'tiving', using template directory '/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/templates/project', created in:\r\n",
      "    /home/ubuntu/notebook/크롤링프로젝트/tiving\r\n",
      "\r\n",
      "You can start your first spider with:\r\n",
      "    cd tiving\r\n",
      "    scrapy genspider example example.com\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject tiving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 크롤링 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import scrapy\n",
    "import json\n",
    "\n",
    "from scrapy.http import TextResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apiKey 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.tving.com/pick/player/top/top'\n",
    "req = requests.get(url)\n",
    "response = TextResponse(req.url, body=req.text, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1e7952d0917d6aab1f0293a063697610'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apiKey = response.text.split(\"apiKey=\")[1].split(\"&\")[0]\n",
    "apiKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apiKey로 데이터 수집하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200 http://api.tving.com/v2/media/pick/clips?screenCode=CSSD0100&networkCode=CSND0900&osCode=CSOD0900&teleCode=CSCD0900&apiKey=1e7952d0917d6aab1f0293a063697610&pageNo=1&pageSize=100&isuse=Y&order=viewDay&personal=Y>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page, page_size = 1, 100\n",
    "\n",
    "url = 'http://api.tving.com/v2/media/pick/clips?\\\n",
    "screenCode=CSSD0100&networkCode=CSND0900&osCode=CSOD0900&teleCode=CSCD0900\\\n",
    "&apiKey={}&pageNo={}&pageSize={}&isuse=Y&order=viewDay&personal=Y'.format(apiKey, page, page_size)\n",
    "\n",
    "req = requests.get(url)\n",
    "response = TextResponse(req.url, body=req.text, encoding='utf-8')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가져온 데이터 파싱 json.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [data['clip_info']\n",
    "         for data in json.loads(response.body_as_unicode())['body']['result']]\n",
    "view_counts = [data['view_count'] for data in json.loads(\n",
    "    response.body_as_unicode())['body']['result']]\n",
    "ids = [data for data in json.loads(response.body_as_unicode())[\n",
    "    'body']['result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "program = items[0]['program']['program_info']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = items[0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "regdate = \"{}.{}.{}\".format(items[0]['regdate'][:4], items[0]['regdate'][4:6], items[0]['regdate'][6:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day': 325,\n",
       " 'week': 347,\n",
       " 'total': 429,\n",
       " 'noParseDay': 2020031505000000325,\n",
       " 'noParseWeek': 2020031505000000347}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_columns = [\"day\", \"week\", \"total\"]\n",
    "view_counts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item, view_count, id in zip(items, view_counts, ids):\n",
    "    program = item['program']['program_info']['title']\n",
    "    title = item['title']\n",
    "    regdate = \"{}.{}.{}\".format(item['regdate'][:4], item['regdate'][4:6], item['regdate'][6:8])\n",
    "    for view_column in view_columns:\n",
    "       view = view_count[view_column]\n",
    "    pid = id['pick_clip_id']\n",
    "    link = 'http://www.tving.com/pick/player/top/'+pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "programs = []\n",
    "\n",
    "for data in json.loads(response.body_as_unicode())['body']['result']:\n",
    "    program = data['clip_info']['program']['program_info']['title'] \n",
    "    programs.append(program)\n",
    "    title = data['clip_info']['title']\n",
    "    regdate = \"{}.{}.{}\".format(data['clip_info']['regdate'][:4],\\\n",
    "                                data['clip_info']['regdate'][4:6],\\\n",
    "                                data['clip_info']['regdate'][6:8])\n",
    "    view_day = data['view_count']['day']\n",
    "    view_week = data['view_count']['week']\n",
    "    view_total = data['view_count']['total']\n",
    "    pid = data['pick_clip_id']\n",
    "    link = 'http://www.tving.com/pick/player/top/'+pid\n",
    "#program, title, regdate, view_day, view_week, view_total\n",
    "len(programs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### items.py 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -*- coding: utf-8 -*-\r\n",
      "\r\n",
      "# Define here the models for your scraped items\r\n",
      "#\r\n",
      "# See documentation in:\r\n",
      "# https://docs.scrapy.org/en/latest/topics/items.html\r\n",
      "\r\n",
      "import scrapy\r\n",
      "\r\n",
      "\r\n",
      "class TivingItem(scrapy.Item):\r\n",
      "    # define the fields for your item here like:\r\n",
      "    # name = scrapy.Field()\r\n",
      "    pass\r\n"
     ]
    }
   ],
   "source": [
    "!cat tiving/tiving/items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tiving/tiving/items.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tiving/tiving/items.py\n",
    "import scrapy\n",
    "\n",
    "class TivingItem(scrapy.Item):\n",
    "    program = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    regdate = scrapy.Field()\n",
    "    view_day = scrapy.Field()\n",
    "    view_week = scrapy.Field()\n",
    "    view_total = scrapy.Field()\n",
    "    link = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spider.py 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tiving/tiving/spiders/spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tiving/tiving/spiders/spider.py\n",
    "import scrapy\n",
    "import json\n",
    "\n",
    "from tiving.items import TivingItem\n",
    "\n",
    "class Spider(scrapy.Spider):\n",
    "    \n",
    "    name = \"Tiving\"\n",
    "    allow_domain = ['tving.com']\n",
    "    start_urls = ['http://www.tving.com/pick/player/top/top']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        apiKey = response.text.split(\"apiKey=\")[1].split(\"&\")[0]\n",
    "        page, page_size = 1, 100\n",
    "        url = 'http://api.tving.com/v2/media/pick/clips?\\\n",
    "screenCode=CSSD0100&networkCode=CSND0900&osCode=CSOD0900&teleCode=CSCD0900\\\n",
    "&apiKey={}&pageNo={}&pageSize={}&isuse=Y&order=viewDay&personal=Y'.format(apiKey, page, page_size)\n",
    "        yield scrapy.Request(url, callback=self.get_content)\n",
    "        \n",
    "    def get_content(self, response):\n",
    "        \n",
    "        for data in json.loads(response.body_as_unicode())['body']['result']:\n",
    "            program = data['clip_info']['program']['program_info']['title'] \n",
    "            title = data['clip_info']['title']\n",
    "            regdate = \"{}.{}.{}\".format(data['clip_info']['regdate'][:4],\\\n",
    "                                data['clip_info']['regdate'][4:6],\\\n",
    "                                data['clip_info']['regdate'][6:8])\n",
    "            view_day = data['view_count']['day']\n",
    "            view_week = data['view_count']['week']\n",
    "            view_total = data['view_count']['total']\n",
    "            pid = data['pick_clip_id']\n",
    "            link = 'http://www.tving.com/pick/player/top/' + pid\n",
    "            \n",
    "            item = TivingItem()\n",
    "            item['program'] = program\n",
    "            item['title'] = title\n",
    "            item['regdate'] = regdate\n",
    "            item['view_day'] = view_day\n",
    "            item['view_week'] = view_week\n",
    "            item['view_total'] = view_total\n",
    "            item['link'] = link\n",
    "            \n",
    "            yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models.py 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tiving/tiving/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tiving/tiving/models.py\n",
    "\n",
    "from sqlalchemy import create_engine, Column, Table, ForeignKey, MetaData\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Integer, Text\n",
    "\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "DeclarativeBase = declarative_base()\n",
    "\n",
    "def db_connect():\n",
    "    \"\"\"\n",
    "    Performs database connection using database settings from settings.py.\n",
    "    Returns sqlalchemy engine instance\n",
    "    \"\"\"\n",
    "    return create_engine(get_project_settings().get(\"CONNECTION_STRING\"))\n",
    "\n",
    "def create_table(engine):\n",
    "    DeclarativeBase.metadata.create_all(engine)\n",
    "    \n",
    "class TvingClipDB(DeclarativeBase):\n",
    "    __tablename__ = \"tvingclip\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    program = Column('program', Text)\n",
    "    title = Column('title', Text)\n",
    "    regdate = Column('regdate', Text)\n",
    "    view_day = Column('view_day', Text)\n",
    "    view_week = Column('view_week', Text)\n",
    "    view_total = Column('view_total', Text)\n",
    "    link = Column('link', Text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline.py 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -*- coding: utf-8 -*-\r\n",
      "\r\n",
      "# Define your item pipelines here\r\n",
      "#\r\n",
      "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\r\n",
      "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\r\n",
      "\r\n",
      "\r\n",
      "class TivingPipeline(object):\r\n",
      "    def process_item(self, item, spider):\r\n",
      "        return item\r\n"
     ]
    }
   ],
   "source": [
    "! cat tiving/tiving/pipelines.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tiving/tiving/pipelines.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tiving/tiving/pipelines.py\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from tiving.models import TvingClipDB, db_connect, create_table\n",
    "\n",
    "class TivingPipeline(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes database connection and sessionmaker.\n",
    "        Creates deals table.\n",
    "        \"\"\"\n",
    "        engine = db_connect()\n",
    "        create_table(engine)\n",
    "        self.Session = sessionmaker(bind=engine)\n",
    "        \n",
    "    def process_item(self, item, spider):\n",
    "        \"\"\"Save deals in the database.\n",
    "\n",
    "        This method is called for every item pipeline component.\n",
    "        \"\"\"\n",
    "        session = self.Session()\n",
    "        tvingclipdb = TvingClipDB()\n",
    "        tvingclipdb.program = item[\"program\"]\n",
    "        tvingclipdb.title = item[\"title\"]\n",
    "        tvingclipdb.regdate = item[\"regdate\"]\n",
    "        tvingclipdb.view_day = item[\"view_day\"]\n",
    "        tvingclipdb.view_week = item[\"view_week\"]\n",
    "        tvingclipdb.view_total = item[\"view_total\"]\n",
    "        tvingclipdb.link = item[\"link\"]\n",
    "\n",
    "        try:\n",
    "            session.add(tvingclipdb)\n",
    "            session.commit()\n",
    "        except:\n",
    "            session.rollback()\n",
    "            raise\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### settings.py 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Obey robots.txt rules\r\n",
      "ROBOTSTXT_OBEY = True\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 22 tiving/tiving/settings.py | tail -n 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sed -i 's/ROBOTSTXT_OBEY = True/ROBOTSTXT_OBEY = False/' tiving/tiving/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Obey robots.txt rules\r\n",
      "ROBOTSTXT_OBEY = False\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 22 tiving/tiving/settings.py | tail -n 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings.py에서 파이프라인 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"ITEM_PIPELINES = {\" >> tiving/tiving/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"    'tiving.pipelines.TivingPipeline': 300,\" >> tiving/tiving/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"}\" >> tiving/tiving/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITEM_PIPELINES = {\r\n",
      "    'tiving.pipelines.TivingPipeline': 300,\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "! tail -n 3 tiving/tiving/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL 설정 \n",
    "#CONNECTION_STRING = \"{drivername}://{user}:{passwd}@{host}:{port}/{db_name}?charset=utf8mb4\".format(\n",
    "#     drivername=\"mysql\",\n",
    "#     user=\"root\",\n",
    "#     passwd=\"password\",\n",
    "#     host=\"**.***.**.***\",\n",
    "#     port=\"****\",\n",
    "#     db_name=\"OTT\",\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run.sh 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "cd tiving\n",
    "scrapy crawl Tiving -o tiving_04.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chmod(\"run.sh\", 0o764)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxrw-r-- 1 ubuntu ubuntu 43 Mar 15 17:28 run.sh\r\n"
     ]
    }
   ],
   "source": [
    "! ls -al run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-02 10:32:17 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: tiving_vod)\n",
      "2020-04-02 10:32:17 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.6.9 (default, Jan 17 2020, 08:43:56) - [GCC 7.4.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Linux-4.15.0-1060-aws-x86_64-with-debian-buster-sid\n",
      "2020-04-02 10:32:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'tiving_vod', 'FEED_FORMAT': 'csv', 'FEED_URI': 'tiving_vod.csv', 'NEWSPIDER_MODULE': 'tiving_vod.spiders', 'SPIDER_MODULES': ['tiving_vod.spiders']}\n",
      "2020-04-02 10:32:17 [scrapy.extensions.telnet] INFO: Telnet Password: 079106f7cd9513ce\n",
      "2020-04-02 10:32:17 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-04-02 10:32:17 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-04-02 10:32:17 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "Unhandled error in Deferred:\n",
      "2020-04-02 10:32:17 [twisted] CRITICAL: Unhandled error in Deferred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/crawler.py\", line 184, in crawl\n",
      "    return self._crawl(crawler, *args, **kwargs)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/crawler.py\", line 188, in _crawl\n",
      "    d = crawler.crawl(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1613, in unwindGenerator\n",
      "    return _cancellableInlineCallbacks(gen)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1529, in _cancellableInlineCallbacks\n",
      "    _inlineCallbacks(None, g, status)\n",
      "--- <exception caught here> ---\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1418, in _inlineCallbacks\n",
      "    result = g.send(result)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/crawler.py\", line 86, in crawl\n",
      "    self.engine = self._create_engine()\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/crawler.py\", line 111, in _create_engine\n",
      "    return ExecutionEngine(self, lambda _: self.stop())\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/engine.py\", line 70, in __init__\n",
      "    self.scraper = Scraper(crawler)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/scraper.py\", line 71, in __init__\n",
      "    self.itemproc = itemproc_cls.from_crawler(crawler)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/middleware.py\", line 53, in from_crawler\n",
      "    return cls.from_settings(crawler.settings, crawler)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/middleware.py\", line 35, in from_settings\n",
      "    mw = create_instance(mwcls, settings, crawler)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/misc.py\", line 146, in create_instance\n",
      "    return objcls(*args, **kwargs)\n",
      "  File \"/home/ubuntu/notebook/Project_crawling/tiving_vod/tiving_vod/pipelines.py\", line 11, in __init__\n",
      "    engine = db_connect()\n",
      "  File \"/home/ubuntu/notebook/Project_crawling/tiving_vod/tiving_vod/models.py\", line 15, in db_connect\n",
      "    return create_engine(get_project_settings().get(\"CONNECTION_STRING\"))\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/sqlalchemy/engine/__init__.py\", line 479, in create_engine\n",
      "    return strategy.create(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py\", line 56, in create\n",
      "    plugins = u._instantiate_plugins(kwargs)\n",
      "builtins.AttributeError: 'NoneType' object has no attribute '_instantiate_plugins'\n",
      "\n",
      "2020-04-02 10:32:17 [twisted] CRITICAL: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1418, in _inlineCallbacks\n",
      "    result = g.send(result)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/crawler.py\", line 86, in crawl\n",
      "    self.engine = self._create_engine()\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/crawler.py\", line 111, in _create_engine\n",
      "    return ExecutionEngine(self, lambda _: self.stop())\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/engine.py\", line 70, in __init__\n",
      "    self.scraper = Scraper(crawler)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/scraper.py\", line 71, in __init__\n",
      "    self.itemproc = itemproc_cls.from_crawler(crawler)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/middleware.py\", line 53, in from_crawler\n",
      "    return cls.from_settings(crawler.settings, crawler)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/middleware.py\", line 35, in from_settings\n",
      "    mw = create_instance(mwcls, settings, crawler)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/misc.py\", line 146, in create_instance\n",
      "    return objcls(*args, **kwargs)\n",
      "  File \"/home/ubuntu/notebook/Project_crawling/tiving_vod/tiving_vod/pipelines.py\", line 11, in __init__\n",
      "    engine = db_connect()\n",
      "  File \"/home/ubuntu/notebook/Project_crawling/tiving_vod/tiving_vod/models.py\", line 15, in db_connect\n",
      "    return create_engine(get_project_settings().get(\"CONNECTION_STRING\"))\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/sqlalchemy/engine/__init__.py\", line 479, in create_engine\n",
      "    return strategy.create(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py\", line 56, in create\n",
      "    plugins = u._instantiate_plugins(kwargs)\n",
      "AttributeError: 'NoneType' object has no attribute '_instantiate_plugins'\n"
     ]
    }
   ],
   "source": [
    "! ./run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
